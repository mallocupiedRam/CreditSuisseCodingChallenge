Jacob Brown

Contact No: +91-8830654648

Current Location: Pune

E-Mail: jacobbrown@gmail.com

Career Objective    

To work in an organization, where professionalism & enthusiasm are recognized and appreciated. To contribute strongly to the performance of the organization, deliver success through a unique drive for excellence. 

Professional Summary    

Over 2.6 Years as Associate Consultant, professional IT experience in Data Analysis which includes DWH/Big data in Capgemini Pune.
Hands on experience onETL and Big data Tools such as Datastage, Informatica -ETL development, Talend, Teradata components such asDesigner, Workflow Manager, Hadoop stack, HDFS, Map Reduce.
Have good knowledge of Talend 6.3.1 ETL and IBM Rational Quality Manager.
Good Exposure on Apache HadoopMap ReduceprogrammingPIG Scriptingand Distribute Application andHDFS.
Expertise and well versed with various  Datastage and Talend Transformationss, Partition, De partition , Dataset and Database components.
Analyzing the source data to know the quality of data by using Talend Data Quality.
Composing TWS schedule files and executing code through schedules.
Designed various DataStage jobs and Sequencers to load data into Teradata using various source files
Worked with Data mapping team to understand the source to target mapping rules
Worked closely with the development and DA team to understand the new changes in the design & accommodate the same in the test strategy.
Involved in Defect Status, Meeting and Walkthrough along with Development, clients and Team members.
Designed various Talend jobs to load data from Teradata to AWS Redshift. 
Involved in Training sessions & Knowledge transfer sessions for new joiners.
Quick learner and excellent team player having ability to meet tight deadlines and work under pressure.



Project Details



1.

PROJECT PROFILE

Project Name

GDAP Migration and GDAP Operations Support

Duration

July 2017 to till date

Roles and Responsibilities

This project aim is to migrate the existing GDAP Application from Data Stage to Talend and Teradata to Redshift. 
Created Talend ETL jobs to receive attachment files from pop e-mail using tPop, tFileList and tFileInputMail and then loaded data from attachments into database and achieved the files.
Responsible for creation of Talend batch jobs for daily jobs and history load, Source SQL Query, Ensuring compliance of business Standards.
Designing,Planning,Debugging and maintaining unit test case, data validation and integration testing.
Setting up the environment and executing the scripts and testing part.
Testing the data from source to target in AWS Redshift as source and raising defects whenever required
Involved in loading data fromUNIX file systemtoHDFS.
 Provided deployment support during release. 

Tools Used

Talend6.3, Redshift,S3, Hadoop HDFS,Unix



2.

PROJECT PROFILE

Project Name

McDonalds Reporting and Break fix

Duration

Mar 2017 to July 2017

Roles and Responsibilities

Data Warehouse Testing in Waterfall environment
Testing the data of tables using toad.
Validating the source data from Datastage to tables in Teradata.
In-depth knowledge of UNIX shell scripting
Testing the data from source to target in toad and then presenting same into SSRS reports.
Initiating offshore team meetings for process improvements. 
Develop automation scripts for reducing manual efforts.

Tools Used

Teradata, Putty, Datastage, sql,Unix Shell Scripting



3.

PROJECT PROFILE

Project Name

McDonalds GDW BUILD Analytics

Duration

Oct 2016- Feb 2017

Roles and Responsibilities

This project aim is to Design and Develop Data Stage jobs to load data into Teradata as per requirement. 
Working as offshore SPOC, responsible for derivable, WSR report and status call with all stake holders.
Planning, Debugging and maintaining it in test plan.
Involved in the impact analysis of the existing report, Creating Mapping documents, HLD and LLD
Setting up the environment and executing the scripts using Teradata multiple utilities and testing part.
Testing the data from source to target in sql developer, Teradata  
Creation of Teradata SQL Scripts, UNIX Shell Scripts, Python
Raising defects wherever required. Documenting test summary.
Deep knowledge of Informatica interaction among these components
Provided deployment support during release. 

Tools Used

Teradata, Informatica, Datastage, UNIX, Sql Developer, Oracle 



Skills Profile

Operating System 

Windows 7/8/10, LINUX 

Languages

C,C++Unix Shell Scripting, Python

Database 

Oracle 11g, SQL, Teradata, Redshift 

ETL tools

Talend 6.3,IBM DataStage 11.3,

3 Months Training

SSIS, SSRS, Datastage, Unix, SQL,PLSQL



Other Information:

Have conducted SQL and automation training for freshers.

Worked on Automation mentioned below:

Developed script in Unix  for DDL comparison of tables.

Developed script in Unix for comparing count of Oracle source and Teradata tables.

Developed script in Unix for comparing Testing and Development scripts.



Education



Year

Qualification

Institute

%Marks

2016

B.E. in Computer Engineering

Sinhgad Academy of Engineering Pune

72.8



Academic Achievements

Class Representative of my class in College.

Received Certificate of Recognition in Capgemini within 1 year.

Got Recognition award for good performance

Got promoted to Associate Consultant within 2 years of my career.

Awarded Pat on Back Award and Star of the Project for outstanding performance in current project



Declaration: - The above statements are true to the best of my knowledge and belief.